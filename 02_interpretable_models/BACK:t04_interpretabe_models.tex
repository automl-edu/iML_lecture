\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Introduction]{iML: Introduction}
\subtitle{Interpretable Models}
%\institute{}

\begin{document}
	
	\maketitle

\begin{frame}{Interpretable Models}
	\textbf{Linear models (LM) and generalized linear models (GLM):}
	
	The specification of model parameters makes LMs and GLMs intrinsically interpretable.
	
	\begin{itemize}
		
		\item \textbf{LM:}
		$$
		\mathbb{E}_Y(y \vert x) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \epsilon
		$$
		
		\item \textbf{GLM:}
		$$
		g\left(\mathbb{E}_Y(y \vert x)\right) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p
		$$
	\end{itemize}
	
	\pause
	\textbf{Generalized additive models (GAM):} Add flexibility by replacing linear term by more general functional form, but retain interpretability by keeping the pre-specified additive predictor.
	
	$$
	g\left(\mathbb{E}_Y(y \vert x)\right) = \beta_0 + \beta_1 h(x_1) + \dots + \beta_p h(x_p)
	$$
	
\end{frame}


\begin{frame}{Interpretable Models}
	
	
	\textbf{Model-based boosting:}
	
	\begin{itemize}
		
		\item Idea: Combine boosting with interpretable base learners\\
		 (e.g., linear model with single non-zero parameter).
		\pause
		\item Consider two linear base learners $b_j(x, \Theta)$ and $b_j(x, \Theta^{\star})$ with the same type, but distinct parameter vectors $\Theta$ and $\Theta^{\star}$. They can be combined in a base learner of the same type:
		
		$$
		b_j(x, \Theta) + b_j(x, \Theta^{\star}) = b_j(x, \Theta + \Theta^{\star})
		$$
		\pause
		\item We iteratively create a selection of interpretable base learners. In each iteration:
		\begin{enumerate}
			\item all base learners are trained on the so-called pseudo residuals,
			\item the one with the best fit is added to the previously computed model.
		\end{enumerate}  
		\pause
		\item The final model has an additive structure (equivalent to a GAM),\\ where each component function is itself interpretable.
	\end{itemize}
	
	
	
\end{frame}

\begin{frame}{Interpretable Models}
	
	
	\textbf{Rule-based ML:}
	
	
	Decision rules follow a general structure: IF the conditions are met THEN make a certain prediction. A single decision rule or a combination of several rules can be used to make predictions.
	
	\vspace{0.5cm}
	There are many ways to learn rules from data:
	\begin{itemize}
		\item OneR ("One Rule") learns rules from a single feature. OneR is characterized by its simplicity, interpretability and its use as a benchmark.
		\pause
		\item "Sequential covering" is a general procedure that iteratively learns rules and removes the data points that are covered by the new rule. %This procedure is used by many rule learning algorithms.
		\pause
		\item "Bayesian Rule Lists" combine pre-mined frequent patterns into a decision list using Bayesian statistics. %Using pre-mined patterns is a common approach used by many rule learning algorithms.
	\end{itemize}
	
\end{frame}
	
\end{document}
