\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[Introduction]{iML: Introduction}
\subtitle{fANOVA: Additive Decomposition}
%\institute{}
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}


\begin{document}
	
	\maketitle

\begin{frame}[c]{High-Dimensional Model Representation}
	
	\begin{itemize}
		\item
		A high-dimensional model representation (HDMR) decomposes the model into a sum of effect terms of increasing order:
		\begin{align*}
		\hat{f}(x) &= g_{\{0\}} + g_{\{1\}}(x_1) + g_{\{2\}}(x_2) + \;\dots\; + g_{\{1, 2\}}(x_1, x_2) \\
		&\phantom{{}={}} + \;\dots\; + g_{\{1,\ldots,p\}}(x_1, \ldots,x_p)
		\end{align*}
		\pause
		\item The features need to be independent to make the HDMR unique.
		\pause
		\item Different techniques to estimate an additive decomposition exist, e.g., 
		\begin{itemize}
			\item repeated expectations (partial dependence / PD) 
			\item  accumulated local effects (ALE).
		\end{itemize}
		
	\end{itemize}
\end{frame}

\begin{frame}[c]{Additive Decomposition of a Prediction Function}
	
	Consider the estimation via iterative expectations:
	\begin{align*}
	g_{\{0\}} &= \mathbb{E}_X\left[\widehat{f}(x)\right] \\
	g_{\{1\}}(x_1) &= \mathbb{E}_{X_{-1}}\left[\widehat{f}(x) \; \vert  \; X_1 \right] - g_{\{0\}} \\
	g_{\{2\}}(x_2) &= \mathbb{E}_{X_{-2}}\left[\widehat{f}(x) \; \vert  \; X_2 \right] - g_{\{0\}} \\
	g_{\{1, 2\}}(x_1, x_2) &= \mathbb{E}_{X_{-\{1,2\}}}\left[\widehat{f}(x) \; \vert \; X_1, X_2 \right] - g_{\{2\}}(x_2) - g_{\{1\}}(x_1) - g_{\{0\}}\\
	&\vdots \\
	g_{\{1, \dots, p\}}(x) &= \widehat{f}(x) - \ldots - g_{\{1, 2\}}(x_1, x_2) \\
	&\phantom{{}={}} - g_{\{2\}}(x_2) - g_{\{1\}}(x_1) - g_{\{0\}}\\
	\end{align*}
	
\end{frame}

\begin{frame}[c]{Functional ANOVA}
	
	After $\hat{f}$ has been decomposed, we can conduct a functional analysis of variance (fANOVA):
		\begin{align*}
		Var\left[\hat{f}(x)\right] &= Var\left[g_{\{0\}} + g_{\{1\}}(x_1) + g_{\{2\}}(x_2) + \;\dots\; + g_{\{1, 2\}}(x_1, x_2) \right. \\
		&\phantom{{}={}} \left. + \;\dots\; + g_{\{1,\ldots,p\}}(x) \right]
		\end{align*}
		\pause
		
		If the features are independent, the variance can be additively decomposed without covariances:
		\begin{align*}
		Var\left[\hat{f}(x)\right] &= Var\left[g_{\{0\}}\right] + Var\left[g_{\{1\}}(x_1)\right] + Var\left[g_{\{2\}}(x_2)\right] \\
		&\phantom{{}={}} + Var\left[g_{\{1, 2\}}(x_1, x_2)\right] + \;\dots\; + Var\left[g_{\{1,\ldots,p\}}(x)\right]
		\end{align*}
\end{frame}
		
\begin{frame}[c]{Functional ANOVA (cont'd)}
		Dividing by the prediction variance results in the fraction of variance explained by each term:
		\begin{align*}
		1 &= \frac{Var\left[g_{\{0\}}\right]}{\predvar} + \frac{Var\left[g_{\{1\}}(x_1)\right]}{\predvar} + \frac{Var\left[g_{\{2\}}(x_2)\right]}{\predvar} \\
		&\phantom{{}={}} + \frac{Var\left[g_{\{1, 2\}}(x_1, x_2)\right]}{\predvar} + \;\dots\; + \frac{Var\left[g_{\{1,\ldots,p\}}(x)\right]}{\predvar}
		\end{align*}
	%	\pause
		
	%	The fraction of variance explained by a term is referred to as the Sobol index:
	%	$$
%		S_j = \frac{Var\left[g_{\{j\}}(x_j)\right]}{Var\left[\hat{f}(x)\right]}
%		$$
	
\end{frame}


\end{document}
